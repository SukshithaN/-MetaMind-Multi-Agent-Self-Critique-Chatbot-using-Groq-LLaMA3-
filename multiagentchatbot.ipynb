{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "MODEL = \"llama3-8b-8192\"\n",
        "client = None  # Will be initialized after uploading API key\n",
        "\n",
        "\n",
        "# Agent Functions\n",
        "def agent1(prompt):\n",
        "    return query_groq(prompt, system=\"You are a helpful AI assistant that gives direct answers.\")\n",
        "\n",
        "\n",
        "def agent2(response):\n",
        "    return query_groq(response, system=\"Critique the previous answer and explain its flaws if any.search for flaws likehallucinatio, logical errors and factual erroesand reasoning mistakes and others usual errors that are done by llms\")\n",
        "\n",
        "\n",
        "def agent3(original, critique):\n",
        "    return query_groq(f\"Original answer: {original}\\nCritique: {critique}\\nImprove the answer based on the critique.\",\n",
        "                      system=\"Revise the answer based on critique, give more clarity and accuracy.\")\n",
        "\n",
        "\n",
        "def query_groq(prompt, system):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"[ERROR] {str(e)}\"\n",
        "\n",
        "\n",
        "# Main Chat Function\n",
        "def chat(prompt, api_key):\n",
        "    global client\n",
        "    try:\n",
        "        client = Groq(api_key=api_key.strip())\n",
        "\n",
        "        a1 = agent1(prompt)\n",
        "        a2 = agent2(a1)\n",
        "        a3 = agent3(a1, a2)\n",
        "\n",
        "        return a1, a2, a3\n",
        "    except Exception as e:\n",
        "        return f\"[ERROR] {str(e)}\", \"\", \"\"\n",
        "\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"### ü§ñ MetaMind ‚Äì Multi-Agent Self-Critique Chatbot (Groq + LLaMA3-8B)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        groq_key = gr.Textbox(label=\"üîë Enter your Groq API Key\", type=\"password\")\n",
        "        prompt = gr.Textbox(label=\"üß† Your Prompt\", placeholder=\"Ask anything...\")\n",
        "\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "    clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "    with gr.Row():\n",
        "        a1_output = gr.Textbox(label=\"üí¨ Agent 1 ‚Äì Initial Answer\")\n",
        "        a2_output = gr.Textbox(label=\"üßê Agent 2 ‚Äì Critique\")\n",
        "        a3_output = gr.Textbox(label=\"üõ† Agent 3 ‚Äì Improved Answer\")\n",
        "\n",
        "    submit_btn.click(chat, inputs=[prompt, groq_key], outputs=[a1_output, a2_output, a3_output])\n",
        "    clear_btn.click(lambda: (\"\", \"\", \"\", \"\"), inputs=[], outputs=[prompt, a1_output, a2_output, a3_output])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "U0Yiw2Gze3hq",
        "outputId": "1593344b-0349-4a54-ad74-38ec1f543cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b5fd6ae5d920cb1cad.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b5fd6ae5d920cb1cad.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}